# How-to for GROMACS/PLUMED installation and submission on CSCS Alps Daint
This is a short summary to compile your own GROMACS version on [CSCS Alps](https://www.cscs.ch/computers/alps). We will (hopefully) get access to the pool of ca. 2700 [NVIDIA Grace-Hopper](https://www.nvidia.com/en-us/data-center/grace-hopper-superchip/) (GH) nodes on Daint. Each GH chipset has 72 cores, 128GB RAM, and a H100 GPU with 96GB of memory. Keep in mind that these are massive nodes, as each one has four GH chipsets, in contrast to Piz-Daint, which feautured small nodes and good internode scaling. This has a few consequences
1. We cant (as for now) require just a fraction of a node. You have to require a full one, even if you plan to run only on one of the chipsets, and consequently will burn the full computational time you require even if you use only part of the machine.
2. The amount of cores and the GPU are overpowered for most unbiased MD simulations of small (<100k atoms) systems. It is very likely that in these cases the scaling is negative, with performance decreasing the moment you split the job on more than one chipset. For small sized jobs, or jobs with only one replica, baobab is still the best option.
3. As far as what we do now, it is highly unlikely that we will ever scale beyond one node, as the GH chipsets are so powerful that internode communication becomes a serius bottleneck. As such, countrary to Piz-Daint, most of our jobs here will be only single-node jobs.
Regarding our common types of simulations, the main take at home messages are
1. For standard OneOpes with 8 replicas and ca. 100/200k atoms boxes, a node is a good compromise and should outperform Baobab (at least in terms of stability). You can easily put two replicas per GH chipset, effectively giving half a GPU and 32 cores to each replica. More on this later on.
2. For small OneOpes runs (e.g. folding), Baobab will be a better choice, as the systems are so small that you canâ€™t really benefit from GH chipsets. It might be that running all eight replicas on the same chipset might be the best choice, but this will leave three-quaters of the node empty. More testing on this in the future probably will be required if needed.
3. For unbiased simulations, test a few configurations and see how the system scales. Most likely, the best choice will be to run in parallel two or four simulations (2 GPUs or 1 GPU per sim, respectively) by using the multidir flag, but without exchanges. Also more on this later on.
It is likely that in the future we will compile a shared version of GROMACS and PLUMED for everyone to source. For the time being, if you need Daint you will likely need to compile your GROMACS and PLUMED versions. First, login to Daint (more info [`here`](https://confluence.cscs.ch/display/KB/Daint), for standard ssh you still jump through ela, the new address is daint.alps.cscs.ch). With respect to Piz-Daint and Baobab, here there is not the module load syntax anymore, but they use uenv to set up the user environment. Basically, you have to create a container that holds your environments, check which environments are available, and pull the one you need. This needs to be done only once, then the environment remains available without having to pull it again. To create the container and pull GROMACS environment just run
